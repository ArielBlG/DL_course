{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pJx6Dv0s5umq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "import time\n",
    "import csv\n",
    "sns.set()\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ASrnEsAs54ea"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'lfw2'\n",
    "TRAIN_PATH = 'pairsDevTrain.txt'\n",
    "TEST_PATH = 'pairsDevTest.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6jfbmUG-56mw"
   },
   "outputs": [],
   "source": [
    "def get_rows(path_train, path_test):\n",
    "    \"\"\"\n",
    "    Get the rows of the train and test data\n",
    "    :param path_train:\n",
    "    :param path_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(path_train, 'r') as csvfile:\n",
    "        trainrows = list(csv.reader(csvfile, delimiter='\\t'))[1:]\n",
    "\n",
    "    with open(path_test, 'r') as csvfile:\n",
    "        testrows = list(csv.reader(csvfile, delimiter='\\t'))[1:]\n",
    "\n",
    "    return trainrows, testrows\n",
    "\n",
    "def return_fixed_dataset(df):\n",
    "    \"\"\"\n",
    "    Return the fixed dataset\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for index,row in df.iterrows():\n",
    "        if row[3] is None:\n",
    "          rows.append([row[0], row[1], row[0], row[2]])\n",
    "        else:\n",
    "          rows.append([row[0], row[1], row[2], row[3]])\n",
    "    return rows\n",
    "\n",
    "def split_train_test_val(trainrows, testrows):\n",
    "    \"\"\"\n",
    "    Split the train and test data into train, test and validation sets\n",
    "    :param trainrows:\n",
    "    :param testrows:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_train = pd.DataFrame(trainrows)\n",
    "    df_test = pd.DataFrame(testrows)\n",
    "    print(df_train.shape)\n",
    "    trainrows = return_fixed_dataset(df_train)\n",
    "    testrows = return_fixed_dataset(df_test)\n",
    "    print(len(trainrows))\n",
    "    df_train = pd.DataFrame(trainrows, columns=['name1', 'num1', 'name2', 'num2'])\n",
    "    df_test = pd.DataFrame(testrows, columns=['name1', 'num1', 'name2', 'num2'])\n",
    "    print(df_train.shape)\n",
    "    df_train = df_train.sample(frac=1)\n",
    "    cut_first = int(df_train.shape[0]*0.85)\n",
    "    df_val = df_train[cut_first:]\n",
    "    df_train = df_train[:cut_first]\n",
    "    print(df_val.shape)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "def load_image(name, num, shape):\n",
    "    \"\"\"\n",
    "    Load the image\n",
    "    :param name:\n",
    "    :param num:\n",
    "    :param shape:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        num = int(num)\n",
    "        file_lists = glob(f'{DATA_DIR}/{name}/*')\n",
    "        assert len(file_lists) != 0, \"Shouldn't be empty list!\"\n",
    "        file_lists.sort(key=lambda row: int(row.split('_')[-1][:-4]))\n",
    "        img = cv2.imread(file_lists[num - 1], cv2.IMREAD_GRAYSCALE) / 255\n",
    "        if shape is None:\n",
    "            return img\n",
    "        return cv2.resize(img, shape).reshape(105, 105, 1)\n",
    "    except:\n",
    "        print(\"Error in image loading\")\n",
    "\n",
    "def load_pairs(name_1, num_1, name_2, num_2, shape=None):\n",
    "    \"\"\"\n",
    "    Load the pairs of images\n",
    "    :param name_1:\n",
    "    :param num_1:\n",
    "    :param name_2:\n",
    "    :param num_2:\n",
    "    :param shape:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return load_image(name_1, num_1, shape), load_image(name_2, num_2, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Rl3MEYeG5-CG"
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(vect):\n",
    "    \"\"\"\n",
    "    Calculate the euclidean distance\n",
    "    :param vect:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x, y = vect\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    result = K.maximum(sum_square, K.epsilon())\n",
    "    return result\n",
    "\n",
    "\n",
    "def euclidean_distance(vecs):\n",
    "    \"\"\"\n",
    "    Calculate the euclidean distance\n",
    "    :param vecs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x, y = vecs\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def subs_square(vecs):\n",
    "    \"\"\"\n",
    "    Calculate the square of the difference\n",
    "    :param vecs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x, y = vecs\n",
    "    return K.square(x - y)\n",
    "\n",
    "\n",
    "def mae(vecs):\n",
    "    \"\"\"\n",
    "    Calculate the mean absolute error\n",
    "    :param vecs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return K.abs(vecs[0] - vecs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6uPv690P6Fxq"
   },
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape, kernel_initializer, bias_initializer, kernel_regularizer=0.01, kernel_regularizer_dense = 0.0001):\n",
    "    \"\"\"\n",
    "    create the siamese model based on the input arguments\n",
    "    :param input_shape:\n",
    "    :param kernel_initializer:\n",
    "    :param bias_initializer:\n",
    "    :param kernel_regularizer:\n",
    "    :param kernel_regularizer_dense:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (10, 10),\n",
    "                            activation='relu',\n",
    "                            input_shape=input_shape,\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            bias_initializer=bias_initializer,\n",
    "                            kernel_regularizer=l2(kernel_regularizer)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (7, 7), activation='relu',\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            bias_initializer=bias_initializer,\n",
    "                            kernel_regularizer=l2(kernel_regularizer)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (4, 4), activation='relu',\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            bias_initializer=bias_initializer,\n",
    "                            kernel_regularizer=l2(kernel_regularizer)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D())\n",
    "\n",
    "    model.add(layers.Conv2D(256, (4, 4), activation='relu',\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            bias_initializer=bias_initializer,\n",
    "                            kernel_regularizer=l2(kernel_regularizer)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='sigmoid',\n",
    "                           kernel_initializer=kernel_initializer,\n",
    "                            bias_initializer=bias_initializer,\n",
    "                           kernel_regularizer=l2(kernel_regularizer_dense)))\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_network(img_shape, kernel_initializer, bias_initializer):\n",
    "    \"\"\"\n",
    "    build the network based on the previously built model, this is the last phase of the siamese, with the final dense layer\n",
    "    :param img_shape:\n",
    "    :param kernel_initializer:\n",
    "    :param bias_initializer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_img1 = layers.Input(img_shape)\n",
    "    input_img2 = layers.Input(img_shape)\n",
    "\n",
    "    model = get_siamese_model(img_shape, kernel_initializer, bias_initializer)\n",
    "\n",
    "    siamese_model_img1 = model(input_img1)\n",
    "    siamese_model_img2 = model(input_img2)\n",
    "\n",
    "    x = layers.Lambda(mae)([siamese_model_img1, siamese_model_img2])\n",
    "\n",
    "    x = layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer,\n",
    "                     bias_initializer=bias_initializer)(x)\n",
    "\n",
    "    siamese_net = models.Model(inputs=[input_img1, input_img2], outputs=x)\n",
    "    # siamese_net.summary()\n",
    "\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ENKSVcoH6Jvm"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_generator(df, batch_size):\n",
    "    \"\"\"\n",
    "    generator for loading images to the network\n",
    "    :param df:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for start in range(0, len(df), batch_size):\n",
    "            x_batch_left = []\n",
    "            x_batch_right = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(df))\n",
    "            df_train_batch = df[start:end]\n",
    "            for row in df_train_batch.values:\n",
    "                img1, img2 = load_pairs(*row, (105, 105))\n",
    "                label = 1 if row[0] == row[2] else 0\n",
    "                x_batch_left.append(img1)\n",
    "                x_batch_right.append(img2)\n",
    "                y_batch.append(label)\n",
    "            x_batch_left, x_batch_right, y_batch = np.asarray(x_batch_left), np.asarray(x_batch_right), np.asarray(y_batch)\n",
    "            yield [x_batch_left, x_batch_right], y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bh5Pyo0e6Ms1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(lr, decay_rate, optimizer, batch_size, generator, train_set, val_set, siamese_net, epochs, patience):\n",
    "    \"\"\"\n",
    "    train the network\n",
    "    :param lr:\n",
    "    :param decay_rate:\n",
    "    :param optimizer:\n",
    "    :param batch_size:\n",
    "    :param generator:\n",
    "    :param train_set:\n",
    "    :param val_set:\n",
    "    :param siamese_net:\n",
    "    :param epochs:\n",
    "    :param patience:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = SGD(lr=0.001, momentum=0.5)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=ExponentialDecay(lr, 100000, decay_rate))\n",
    "\n",
    "    train_generator = generator(train_set, batch_size)\n",
    "    val_generator = generator(val_set, batch_size)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', verbose=1, patience=patience, restore_best_weights=True)\n",
    "\n",
    "    siamese_net.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    history = siamese_net.fit(train_generator,\n",
    "                              callbacks=[early_stop],\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=(len(val_set) // batch_size) + 1,\n",
    "                              verbose=0,\n",
    "                              steps_per_epoch=(len(train_set) // batch_size) + 1,\n",
    "                              shuffle=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pInALACr6O7Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(2200, 4)\n",
      "2200\n",
      "(2200, 4)\n",
      "(330, 4)\n",
      "(1870, 4)\n",
      "(1870, 4)\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "trainrows, testrows = get_rows(TRAIN_PATH, TEST_PATH)\n",
    "df_train, df_val, df_test = split_train_test_val(trainrows, testrows)\n",
    "print(df_train.shape)\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ezVBZZpy6RL4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                      | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing hyperparameter tuning...\n",
      "**************************************************\n",
      "Training with learning_rate: 0.001, batch-size: 128, epochs: 10, patience: 10, optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                      | 0/2 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[128,128,42,42] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/sequential/batch_normalization_1/FusedBatchNormV3 (defined at <ipython-input-13-461f6d5d744f>:27) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_168201]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-68b696a43399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m                         f'''Training with learning_rate: {l}, batch-size: {bs}, epochs: {ep}, patience: {pat}, optimizer: {opt}''')\n\u001b[0;32m     46\u001b[0m                     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                     \u001b[0mtest_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-461f6d5d744f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(lr, decay_rate, optimizer, batch_size, generator, train_set, val_set, siamese_net, epochs, patience)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0msiamese_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     history = siamese_net.fit(train_generator,\n\u001b[0m\u001b[0;32m     28\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[128,128,42,42] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/sequential/batch_normalization_1/FusedBatchNormV3 (defined at <ipython-input-13-461f6d5d744f>:27) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_168201]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Runing hyperparameter tuning...\")\n",
    "lr = [0.001, 0.0001]\n",
    "batch_size = [128]\n",
    "epochs = [10, 30, 50]\n",
    "patience = [ 10, 20]\n",
    "optimizer = ['Adam', 'SGD']\n",
    "results = {'lr': [], 'batch_size': [], 'epochs': [], 'patience': [], 'optimizer': []\n",
    "    , 'loss': [], 'accuracy': []}\n",
    "\n",
    "network_params = {\n",
    "    'img_shape': (105, 105, 1),\n",
    "    'kernel_initializer': RandomNormal(mean=0, stddev=0.01),\n",
    "    'bias_initializer': RandomNormal(mean=0.5, stddev=0.01),\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'lr': 0.0001,\n",
    "    'decay_rate': 0.98,\n",
    "    'optimizer': 'Adam',\n",
    "    'batch_size': 64,\n",
    "    'generator': train_generator,\n",
    "    'train_set': df_train,\n",
    "    'val_set': df_val,\n",
    "    'siamese_net': None,\n",
    "    'epochs': 30,\n",
    "    'patience': 5\n",
    "}\n",
    "\n",
    "for l in tqdm(lr):\n",
    "    model_params['lr'] = l\n",
    "    \n",
    "    for bs in batch_size:\n",
    "        model_params['batch_size'] = bs\n",
    "        for ep in epochs:\n",
    "            model_params['epochs'] = ep\n",
    "            for pat in patience:\n",
    "                model_params['patience'] = pat\n",
    "                for opt in optimizer:\n",
    "                    model_params['optimizer'] = opt\n",
    "\n",
    "                    siamese_net = build_network(**network_params)\n",
    "                    model_params['siamese_net'] = siamese_net\n",
    "                    print('*' * 50)\n",
    "                    print(\n",
    "                        f'''Training with learning_rate: {l}, batch-size: {bs}, epochs: {ep}, patience: {pat}, optimizer: {opt}''')\n",
    "                    start = time.time()\n",
    "                    history = train(**model_params)\n",
    "                    test_gen = train_generator(df_test, 64)\n",
    "                    loss, accuracy = siamese_net.evaluate(test_gen, batch_size=64, steps=(len(df_test) // 64) + 1)\n",
    "                    end = time.time()\n",
    "                    print(f\"Training finished after {end - start} time\")\n",
    "                    print(f\"Accuracy: {accuracy}\")\n",
    "                    print(f\"loss: {loss}\")\n",
    "                    print(end - start)\n",
    "                    # print(f\"validation loss: {history.history['val_loss']}\")\n",
    "                    print('*' * 50)\n",
    "                    results['lr'].append(l)\n",
    "                    results['batch_size'].append(bs)\n",
    "                    results['epochs'].append(ep)\n",
    "                    results['patience'].append(pat)\n",
    "                    results['optimizer'].append(opt)\n",
    "                    results['loss'].append(loss)\n",
    "                    results['accuracy'].append(accuracy)\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(results)\n",
    "df_results.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ass2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
